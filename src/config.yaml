environment:
  python:3.10.11

dependencies:
  - pandas==2.2.0
  - nltk==3.8.1
  - tensorflow==2.15.0
  - numpy==1.26.3
  - scikit-learn==1.4.0
  - matplotlib==3.8.2
  - gensim==4.3.2
  - mlflow==2.10.1

setup:
  x_train_sample_size: 200000
  x_val_sample_size: 60000
  x_test_sample_size: 60000
  x_train_sampled_path: data/processed/x_train_sampled.pkl
  x_val_sampled_path: data/processed/x_val_sampled.pkl
  x_test_sampled_path: data/processed/x_test_sampled.pkl
  x_train_cleaned_path: data/processed/x_train_cleaned.pkl
  x_val_cleaned_path: data/processed/x_val_cleaned.pkl
  x_test_cleaned_path: data/processed/x_test_cleaned.pkl
  x_train_lemmatized_path: data/processed/x_train_lemmatized.pkl
  x_val_lemmatized_path: data/processed/x_val_lemmatized.pkl
  x_test_lemmatized_path: data/processed/x_test_lemmatized.pkl
  y_train_sampled_path: data/processed/y_train_sampled.pkl
  y_val_sampled_path: data/processed/y_val_sampled.pkl
  y_test_sampled_path: data/processed/y_test_sampled.pkl
  y_train_preprocessed_path: data/processed/y_train_preprocessed.pkl
  y_val_preprocessed_path: data/processed/y_val_preprocessed.pkl
  y_test_preprocessed_path: data/processed/y_test_preprocessed.pkl


preprocessing:
  tokenizer_max_words: 2000
  max_length: 100
  embedding_dim: 50
  pretrained_embedding_matrix_path: null
  #data/GoogleNews-vectors-negative300.bin
  #data/glove/glove.twitter.27B.50d.txt

cnn:
  epochs: 5
  batch_size: 32
  workers: 3
  figure_path: outputs/history_loss_accuracy.png

bert:
  epochs: 2
  batch_size: 32
  figure_path: outputs/history_loss_accuracy.png